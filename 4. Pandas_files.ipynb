{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Основы работы с функцией read_csv()**\n",
    "\n",
    "Pandas предоставляет простой и удобный интерфейс для работы с табличными данными. Одним из наиболее распространенных форматов данных, с которыми мы сталкиваемся, являются CSV-файлы (Comma-Separated Values), где данные представлены в виде таблицы, разделенной запятыми.  Для этого в Pandas есть функция read_csv(), которую мы будем использовать. Она позволяет загружать данные из файлов CSV и создавать объекты DataFrame, основной структуры данных в Pandas.\n",
    "\n",
    "Синтаксис:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv') # 'data.csv' - путь к файлу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию Pandas сам определяет заголовки (параметр header='infer'), если же они определены неверно, можно указать их явно через параметр header (header=n, где n - номер строки). Если нужно, чтобы Pandas вообще не пытался считать заголовок, надо указать значение параметра header=None.\n",
    "\n",
    "Еще одним важным моментом чтения файлов CSV является указание разделителя столбцов. По умолчанию Pandas ожидает, что столбцы будут разделены запятыми, но иногда файлы CSV могут использовать другие разделители, такие как табуляция или точка с запятой. В таком случае мы можем указать разделитель с помощью **параметра sep**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Параметр encoding** в функции read_csv() в библиотеке Pandas используется для указания кодировки, используемой при чтении файла CSV.\n",
    "\n",
    "Кодировка определяет способ представления символов и текста в файле. Различные языки и регионы могут использовать различные кодировки, и если файл CSV содержит текст, записанный в определенной кодировке, необходимо указать соответствующую кодировку при чтении файла.\n",
    "\n",
    "Параметр encoding принимает строку, содержащую имя кодировки. Некоторые распространенные кодировки, которые могут быть использованы, включают:\n",
    "* `utf-8`: Самая распространенная кодировка для текстовых файлов. Она поддерживает широкий набор символов и является стандартом веба.\n",
    "* `latin-1`: Также известная как ISO-8859-1, она поддерживает большинство европейских языков.\n",
    "* `utf-16`: Кодировка, использующая 16-битное представление символов. Часто используется для мультиязычных данных.\n",
    "* `cp1252`: Расширение кодировки latin-1 с дополнительными символами и символами Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas автоматически определяет кодировку исходя из того, какая кодировка используется в системе (в подавляющем большинстве случаев utf-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Параметр names** в функции read_csv() в библиотеке Pandas используется для явного указания имен столбцов при чтении файла CSV.\n",
    "\n",
    "По умолчанию Pandas ожидает, что первая строка в файле CSV содержит заголовки столбцов. Однако, иногда файлы CSV могут не содержать заголовков, или заголовки могут быть некорректными или неполными. В таких случаях мы можем использовать параметр names для явного указания имен столбцов при чтении файла CSV.\n",
    "\n",
    "Параметр names принимает список или массив строк, где каждая строка представляет собой имя столбца. Количество элементов в списке names должно соответствовать количеству столбцов в файле CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', names=['Имя', 'Возраст', 'Город'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование параметра names особенно полезно при работе с файлами CSV, которые не содержат заголовков или имеют некорректные заголовки. Это позволяет явно задавать имена столбцов и обеспечивает точное соответствие данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Параметр index_col** в функции read_csv() в библиотеке Pandas используется для указания столбца, который следует использовать в качестве индекса (метки строк) при чтении файла CSV.\n",
    "\n",
    "По умолчанию Pandas создает числовой индекс (0, 1, 2, и так далее) для каждой строки DataFrame. Однако, в некоторых случаях удобно использовать значения определенного столбца в качестве индекса, особенно если эти значения являются уникальными и идентифицируют каждую строку набора данных.\n",
    "\n",
    "Параметр index_col может принимать различные значения:\n",
    "\n",
    "* `Целое число`: Если указать целое число, то столбец с соответствующим индексом будет использован в качестве индекса. Например, index_col=0 означает, что первый столбец будет использован в качестве индекса.\n",
    "* `Имя столбца`: Если указать имя столбца (строку), то столбец с этим именем будет использован в качестве индекса. Например, index_col='ID' означает, что столбец с именем \"ID\" будет использован в качестве индекса.\n",
    "* `Список столбцов`: Если указать список столбцов (с целыми числами или именами столбцов), то Pandas будет использовать комбинацию значений из указанных столбцов в качестве составного индекса (мультииндекса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', index_col='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Параметр skiprows** в функции read_csv() в библиотеке Pandas используется для пропуска определенного числа строк при чтении файла CSV.\n",
    "\n",
    "Иногда файлы CSV могут содержать строковые заголовки или метаинформацию, которые не являются частью набора данных, и нам необходимо пропустить эти строки при загрузке данных. В таких случаях мы можем использовать параметр skiprows для указания количества строк, которые следует пропустить.\n",
    "\n",
    "Параметр skiprows принимает целое число или список целых чисел, представляющих номера строк, которые следует пропустить. Нумерация строк начинается с 0, то есть первая строка имеет номер 0, вторая - 1 и так далее. Можно указывать несколько строк для пропуска, передавая их в виде списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', skiprows=1) # пропустится первая строка\n",
    "df = pd.read_csv('data.csv', skiprows=2) # пропустятся первые две строки\n",
    "df = pd.read_csv('data.csv', skiprows=[0, 2]) # пропустятся первая и третья строки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Параметр skipfooter** в функции read_csv() в библиотеке Pandas используется для пропуска определенного числа строк в конце файла CSV при чтении.\n",
    "\n",
    "В некоторых случаях файлы CSV могут содержать строковые или метаданные строки в конце файла, которые не являются частью набора данных и не должны быть загружены. В таких случаях мы можем использовать параметр skipfooter для указания количества строк, которые следует пропустить в конце файла.\n",
    "\n",
    "Параметр skipfooter принимает целое число, которое указывает количество строк, которые следует пропустить. При чтении файла CSV Pandas будет пропускать указанное количество строк в конце файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', skipfooter=2) # пропустятся последние две строки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр skipfooter также имеет дополнительное требование: при использовании этого параметра Pandas не будет использовать быстрый алгоритм чтения CSV-файла, и это может сказаться на производительности при чтении больших файлов. Поэтому рекомендуется быть осторожным при использовании skipfooter с крупными файлами данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Параметр na_values** в функции read_csv() в библиотеке Pandas используется для указания значений, которые следует рассматривать как пропущенные данные (NA, NaN, null) при чтении файла CSV.\n",
    "\n",
    "В файле CSV могут быть явно указанные значения, которые представляют собой пропущенные или отсутствующие данные. Эти значения могут быть обозначены различными способами, например, как \"NA\", \"NaN\", \"null\", или любым другим пользовательским значением.\n",
    "\n",
    "Параметр na_values позволяет указать такие значения, чтобы Pandas мог корректно распознать и обрабатывать их как пропущенные данные при чтении файла CSV. Параметр na_values может принимать различные форматы:\n",
    "\n",
    "* `Строка`: Можно передать строку, содержащую одно или несколько значений, разделенных запятыми или пробелами. Например, na_values='NA' или na_values='NA, --, NULL'.\n",
    "* `Список`: Можно передать список значений, которые следует рассматривать как пропущенные. Например, na_values=['NA', '--', 'NULL'].\n",
    "* `Словарь`: Можно передать словарь, где ключами являются имена столбцов, а значениями - значения, которые следует рассматривать как пропущенные для соответствующих столбцов. Например, na_values={'column1': ['NA', '--'], 'column2': ['NULL']}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', na_values='NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере все значения \"NA\" в файле CSV будут рассматриваться как пропущенные данные и будут заменены на NaN в объекте DataFrame df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Парсинг дат с помощью функции read_csv()**\n",
    "\n",
    "Параметр функции read_csv(), позволяющий парсить даты - parse_dates.\n",
    "\n",
    "Параметр parse_dates может принимать несколько значений:\n",
    "\n",
    "1. Дата в определенной колонке.\n",
    "2. Дата в нескольких колонках.\n",
    "3. Дата в нестандартном формате."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример парсинга даты в определённой колонке\n",
    "df = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# пример парсинга даты в нескольких колонках\n",
    "df = pd.read_csv(\"file.csv\", parse_dates=[\"date\", \"time\"])\n",
    "\n",
    "# пример парсинга даты в нестандартном формате (год-месяц-день-час-минута-секунда)\n",
    "df = pd.read_csv(\"file.csv\", parse_dates=[\"date\"], \n",
    "     date_parser=lambda x: datetime.strptime(x, '%Y-%m-%d-%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция pd.to_datetime() и функция datetime.strptime() модуля datetime, могут использоваться в качестве фиксированных обработчиков дат в параметре date_parser.\n",
    "\n",
    "Подробнее про работу этих функций в файле 'extra'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ещё пример. Допустим, есть csv-файл, содержащий даты в формате 'dd-mm-yyyy'\n",
    "# чтобы прочитать этот файл с помощью параметра date_parser, можно использовать datetime.strptime() и создать функцию, которая принимает дату как строку и возвращает объект datetime\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def custom_date_parser(date_str): \n",
    "    return datetime.strptime(date_str, '%d-%m-%Y') \n",
    "\n",
    "df = pd.read_csv('data.csv', parse_dates=['date'],date_parser=custom_date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример использования функции pd.to_datetime() для парсинга даты из csv_файла\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv') \n",
    "df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y')\n",
    "\n",
    "print(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Чтение больших файлов csv**\n",
    "\n",
    "При работе с большими CSV-файлами может возникнуть проблема из-за ограничений по памяти. С помощью Pandas можно обрабатывать большие файлы, не загружая их полностью в память."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Метод pd.read_csv() с параметром chunksize**\n",
    "\n",
    "Один из способов снижения использования нагрузки на память при работе с большими данными - загрузка и обработка данных по частям. Для этого можно использовать вызов функции pd.read_csv() с параметром chunksize. Параметр chunksize позволяет считывать данные из CSV-файла кусками (chunks) заданного размера. Это может быть особенно полезно, когда нужно обработать огромный по по размеру файл, но ресурсы оперативной памяти ограничены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример синтаксиса и работы параметра chunksize\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"large_file.csv\" \n",
    "chunksize = 1000 # Задаем размер части (куска) \n",
    "# Считываем куски данных из файла \n",
    "chunks = pd.read_csv(filename, chunksize=chunksize) \n",
    "# Обрабатываем каждый кусок \n",
    "for chunk in chunks: \n",
    "    # Здесь можно выполнять любые операции обработки данных \n",
    "    print(chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае chunks, возвращаемый функцией read_csv() является итератором (или генератором), который разбивает весь набор данных из файла, который считывается, на равные части. Каждая часть имеет заданный размер chunksize, что значительно упрощает обработку больших данных с помощью Pandas, так как не нужно загружать весь набор данных в оперативную память за один раз. Каждый раз, когда происходит итерация по объекту chunks, получается часть данных размером chunksize (или меньше, если конец данных) в виде DataFrame. С этим фрагментом данных можно выполнять различные операции, обрабатывая его частями, чтобы работать с большими файлами на слабых компьютерах или при ограниченной доступности памяти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Оптимизация использования памяти**\n",
    "\n",
    "Кроме использования chunksize и iterator, можно оптимизировать потребление памяти при чтении больших CSV-файлов, задавая специфические преобразования и типы данных. Как например, конвертация дат и времени с помощью параметра parse_dates. Это преобразует строковые представления дат и времени в соответствующие объекты `datetime`.\n",
    "\n",
    "Также можно явно задавать типы данных столбцов с помощью параметра dtype. Это особенно полезно для столбцов с категориальными данными или столбцов с большим количеством нулевых значений. Также параметр dtype бывает полезен при большом количестве пропусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример синтаксиса явного задания типов\n",
    "pd.read_csv(\"large_file.csv\", dtype={\"category_column\": \"category\", \"nullable_column\": \"Int64\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Запись данных в файл с помощью функции to_csv()**\n",
    "\n",
    "Функция to_csv в библиотеке Pandas используется для записи данных из объекта DataFrame в файл формата CSV. Она имеет множество параметров, позволяющих настраивать режим записи в файл, включая разделитель, наличие индексов и заголовков, а также многое другое.\n",
    "\n",
    "Основные параметры:\n",
    "- `path_or_buf`: путь к сохраняемому файлу или файлоподобному объекту (file-like), если хотите сохранять данные в буфер памяти\n",
    "- `sep`: разделитель для значений в CSV-файле. По умолчанию ','.\n",
    "- `header`: указывает выводить ли заголовки колонок DataFrame в файл CSV. Если True, заголовок будет выведен. Если False, заголовок не будет выведен. Если передана последовательность строк, то он будет использован в качестве заголовков столбцов вместо отображения названий столбцов DataFrame. По умолчанию True.\n",
    "- `index`: указывает выводить ли индексы в файл CSV. Если True, индексы будут выведены. Если False, индексы не будут выведены. По умолчанию True.\n",
    "- `mode`: режим записи файла, поддерживаемые значения - 'w' (замена), 'a' (дозапись), 'x' (запись в новый файл). По умолчанию 'w'.\n",
    "- `encoding`: кодировка выходного файла. По умолчанию 'utf-8'.\n",
    "- `line_terminator`: строка завершения записи. По умолчанию, как на вашей системе '\\n'.\n",
    "- `date_format`: формат даты и времени для записи, например, '%Y-%m-%d' для год-месяц-день. Если не указан, будут использоваться настройки базы данных и UTC для преобразования дат и времени.\n",
    "- `compression`: Этот параметр позволяет выбрать тип сжатия, например, 'gzip' или 'bz2'. Вышеупомянутые параметры могут быть использованы, чтобы настроить содержимое файла CSV при сохранении DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример экспорта в csv\n",
    "import pandas as pd \n",
    "\n",
    "data = { 'Name': ['Alice', 'Bob', 'Carol'], 'Age': [30, 25, 40], \n",
    "         'Height': [155.2, 175.5, 163.8], 'Weight': [60.5, 80.2, 65.7] } \n",
    "df = pd.DataFrame(data) \n",
    "\n",
    "df.to_csv('my_data.csv', index=False) # сохранится в открытую папку в visual code studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Age,Height,Weight\n",
      "Alice,30,155.2,60.5\n",
      "Bob,25,175.5,80.2\n",
      "Carol,40,163.8,65.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# пример экспорта в буфер обмена\n",
    "import io \n",
    "import pandas as pd\n",
    "\n",
    "data = { 'Name': ['Alice', 'Bob', 'Carol'], 'Age': [30, 25, 40], \n",
    "         'Height': [155.2, 175.5, 163.8], 'Weight': [60.5, 80.2, 65.7] }\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "buffer = io.StringIO()\n",
    "df.to_csv(buffer, index=False)\n",
    "\n",
    "print(buffer.getvalue()) # и скопировать отсюда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Функция pandas.read_table() и ее возможности**\n",
    "\n",
    "Функция pandas.read_table позволяет читать данные из различных форматов файлов. Она может работать с текстовыми файлами, такими как CSV, TSV, TXT, а также с файлами Excel, JSON, HTML и другими.\n",
    "\n",
    "*TSV (Tab-Separated Values) - это формат для хранения табличных данных, в котором столбцы разделены символами табуляции (Tabs), а строки - символами перехода на новую строку. TSV-файлы часто используются для экспорта и импорта данных из различных программ, в частности, они являются стандартным форматом данных в электронных таблицах программ Microsoft Excel и Google Sheets. TSV-файлы подобны CSV-файлам, но вместо запятых в качестве разделителей используется символ табуляции. Использование символа табуляции вместо запятой позволяет создавать более структурированные и читаемые табличные данные, так как обычно запятые могут использоваться внутри значений ячеек в качестве разделителей различных значений.*\n",
    "\n",
    "\n",
    "В целом все форматы файлов, которые можно использовать для хранения структурированных данных, могут быть прочитаны с помощью pandas.read_table.\n",
    "\n",
    "Однако, для корректного парсинга файла с помощью этой функции следует учитывать несколько важных требований:\n",
    "\n",
    "1. Файл должен быть в текстовом формате.\n",
    "2. Файл должен содержать структурированные данные, представленные в виде таблицы.\n",
    "3. Данные в файле должны быть разделены определенным символом - разделителем (обычно символ табуляции, запятая или точка с запятой).\n",
    "4. Для корректного отображения и анализа данных, файл должен содержать заголовок или имена столбцов.\n",
    "5. Файл должен быть легко читаем в буфер памяти, иными словами, размер файла не должен быть слишком большим для системы.\n",
    "\n",
    "Функция pandas.read_table используется для чтения данных из текстовых файлов в формате таблицы, таких как CSV или TSV. Ниже приводятся основные параметры этой функции:\n",
    "\n",
    "- `filepath_or_buffer`: путь к файлу или объекту, содержащему данные для чтения. Этот параметр обязателен.\n",
    "- `sep:` разделитель столбцов в файле. По умолчанию это ‘\\t’ - символ табуляции.\n",
    "- `delimiter`: аналогичен предыдущему параметру.\n",
    "- `header`: номер строки, содержащей имена столбцов. Если не задан, то имена автоматически будут сгенерированы.\n",
    "- `index_col`: номер столбца, который будет использован в качестве индекса таблицы. Если не задан, то индекс будет сгенерирован автоматически.\n",
    "- `usecols`: список столбцов, которые нужно загрузить в таблицу. Если не задан, то загружаются все столбцы.\n",
    "- `dtype`: словарь, указывающий тип данных для каждого столбца. Если не задан, то pandas попытается определить типы данных автоматически.\n",
    "- `na_values`: список значений, которые следует считать отсутствующими в данных.\n",
    "- `parse_dates`: список столбцов, содержащих даты, которые должны быть преобразованы в тип данных pandas datetime.\n",
    "- `skiprows`: количество строк, которые нужно пропустить в начале файла.\n",
    "- `nrows`: количество строк, которые нужно загрузить из файла.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Загрузка JSON файлов в Pandas**\n",
    "\n",
    "JSON (JavaScript Object Notation) - формат обмена данными, основанный на JavaScript. Он представляет собой легкий, текстовый формат, который используется для хранения и передачи данных между клиентом и сервером. Файлы в формате JSON имеют расширение - .json. JSON файлы имеют простую и понятную структуру, которая состоит из двух основных элементов: пар ключ-значение и массивов. Пары ключ-значение представляют собой объекты, которые содержат имена и значения. Массивы представляют собой упорядоченные наборы значений, разделенные запятыми. Ключ-значение представлено в формате:  \"ключ\": \"значение\" (примерно аналогично словарю в Python).\n",
    "\n",
    "Массивы в JSON представлены в формате: \n",
    "\n",
    "`{ \"fruits\": [\"apple\", \"banana\", \"orange\"] }`\n",
    "\n",
    "Это представляет объект fruits, который является массивом из трех элементов: \"apple\", \"banana\", \"orange\". JSON используется практически везде, где нужно обмениваться данными, включая web-приложения, сервисы, базы данных, мобильные приложения и другие программы. В Python для работы с JSON используется стандартная библиотека json. С помощью нее можно конвертировать данные в формате JSON в python объекты и наоборот.\n",
    "\n",
    "Pandas для загрузки данных из JSON файлов предоставляет функцию read_json(). Эта функция загружает данные из файла в формате JSON и создает объект DataFrame. Метод принимает следующие параметры:\n",
    "\n",
    "- `path_or_buf`: имя файла или фалоподобного объекта (например URL-адрес файла из сети Интернет), из которого нужно загрузить данные.\n",
    "- `orient`: определяет ориентацию таблицы (Указывает ожидаемый формата строки JSON). Возможные значения, например: 'split', 'records', 'index', 'columns', 'values'. По умолчанию ориентация определяется автоматически.\n",
    "- `typ`: определяет тип объекта, который нужно создать. Может принимать значения: 'frame', 'series'.\n",
    "- `convert_dates`: определяет нужно ли конвертировать значения в формат datetime.\n",
    "\n",
    "Использование параметра **orient**:\n",
    "- `'columns'`: В этой ориентации каждый ключ верхнего уровня в JSON-файле соответствует столбцу в DataFrame, а значения - это данные в соответствующих столбцах.\n",
    "- `'index'`: В этой ориентации каждый ключ верхнего уровня в JSON-файле соответствует индексу в DataFrame, а значения - это данные в соответствующих строках.\n",
    "- `'records'`: В этой ориентации каждый объект в JSON-файле представляет собой отдельную запись (строку) в DataFrame.\n",
    "- `'split'`: В этой ориентации данные разделены на отдельные части.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Функция pandas.read_html()**\n",
    "\n",
    "HTML (HyperText Markup Language) - это язык разметки, используемый для создания веб-страниц и их структурирования. Он состоит из набора тегов (элементов), которые определяют структуру и содержимое веб-страницы.\n",
    "\n",
    "Основная цель HTML состоит в том, чтобы описывать содержимое веб-страницы, такое как текст, изображения, ссылки, таблицы и другие элементы. Когда браузер открывает HTML-страницу, он интерпретирует и отображает содержимое согласно указанным тегам.\n",
    "\n",
    "основные компоненты:\n",
    "\n",
    "* `<!DOCTYPE html>`: Объявление типа документа, в данном случае HTML5.\n",
    "* `<html>`: Корневой элемент, обозначающий начало и конец HTML-документа.\n",
    "* `<head>`: Содержит метаданные о веб-странице, такие как заголовок (`<title>`) и подключение стилей или скриптов.\n",
    "* `<body>`: Определяет содержимое веб-страницы, отображаемое в браузере.\n",
    "* `<h1>`: Заголовок первого уровня.\n",
    "* `<p>`: Параграф с текстом.\n",
    "* `<img>`: Изображение.\n",
    "* `<a>`: Гиперссылка (ссылка).\n",
    "* `<table>`: Таблица, содержащая строки (`<tr>`) и ячейки (`<td>`).\n",
    "\n",
    "HTML предоставляет широкий набор тегов для создания различных элементов на веб-странице. Каждый тег имеет свое собственное значение и атрибуты, которые могут использоваться для настройки внешнего вида или поведения элемента.\n",
    "\n",
    "Функция read_html() из библиотеки Pandas предназначена для загрузки данных таблиц из HTML-файлов, HTML-страниц или HTML-кода и преобразования их в объекты DataFrame.\n",
    "\n",
    "Функция read_html() из библиотеки Pandas использует парсеры HTML (такие как lxml или beautifulsoup4), чтобы анализировать HTML-код и находить таблицы.\n",
    "\n",
    "Внутри read_html() происходит следующий процесс:\n",
    "\n",
    "1. Функция получает HTML-код из указанного источника (файл, URL или HTML-строка).\n",
    "2. Используя выбранный парсер (по умолчанию это lxml), read_html() анализирует HTML-код и создает объекты, представляющие структуру документа.\n",
    "3. Затем read_html() применяет соответствующие правила и эвристики для определения, какие части HTML-кода считать таблицами.\n",
    "4. Функция ищет теги `<table>` в HTML-коде и анализирует их содержимое, чтобы создать объекты DataFrame, представляющие таблицы.\n",
    "5. Если в HTML-коде присутствует несколько таблиц, read_html() возвращает список объектов DataFrame, каждый из которых соответствует одной таблице.\n",
    "\n",
    "Точность и эффективность работы read_html() может зависеть от структуры и качества HTML-кода. В некоторых случаях, если HTML-код не является хорошо организованным или имеет неточности, может потребоваться дополнительная обработка данных после загрузки таблицы с помощью read_html().\n",
    "\n",
    "**Парсеры HTML** - это программные инструменты, которые анализируют HTML-код и позволяют извлекать данные из него. Они разбирают HTML-документ и создают структурированное объектное представление его содержимого, что облегчает поиск, извлечение и обработку данных.\n",
    "\n",
    "HTML-код представляет собой набор тегов и текстового содержимого, описывающего структуру и содержание веб-страницы. Парсеры HTML сканируют этот код, идентифицируют различные элементы и атрибуты, а затем создают объекты, представляющие эти элементы. Это позволяет программам обрабатывать и взаимодействовать с HTML-кодом.\n",
    "\n",
    "Некоторые из популярных парсеров HTML в языке программирования Python:\n",
    "\n",
    "1. `lxml`: Это быстрый и гибкий парсер, основанный на библиотеке C для обработки XML и HTML. Он предоставляет API для поиска и извлечения данных из HTML-кода, а также для внесения изменений в структуру документа. Библиотека lxml широко используется в Python для парсинга HTML-кода.\n",
    "2. `Beautiful Soup`: Это библиотека Python, предназначенная для извлечения данных из HTML и XML. Она предоставляет простой и интуитивно понятный API, который позволяет выполнять поиск и манипулирование элементами HTML-кода. Beautiful Soup поддерживает разные парсеры, включая lxml, html.parser, html5lib и другие.\n",
    "3. `html.parser`: Это встроенный в стандартную библиотеку Python парсер HTML. Он реализован на языке Python и обеспечивает базовые функции для анализа и обработки HTML-кода. В отличие от некоторых других парсеров, html.parser не требует дополнительной установки.\n",
    "\n",
    "Каждый парсер имеет свои особенности и возможности. Они предоставляют API для поиска, извлечения и манипулирования элементами HTML-кода.\n",
    "\n",
    "**Для использования функции read_html() из библиотеки Pandas требуется установить библиотеки lxml и beautifulsoup4.**\n",
    "\n",
    "Параметры функции read_html():\n",
    "* `io`: Обязательный параметр. Может быть путем к HTML-файлу, URL-адресом или объектом, содержащим HTML-код.\n",
    "* `match`: По умолчанию \".+\". Определяет регулярное выражение для поиска таблиц в HTML-коде. По умолчанию найдутся все таблицы.\n",
    "* `flavor`: По умолчанию None. Указывает на используемую библиотеку для парсинга HTML. Допустимые значения: 'bs4' (BeautifulSoup 4) и 'lxml'.\n",
    "* `header`: По умолчанию None. Указывает номер строки для использования в качестве заголовков столбцов. Если не указано, заголовки будут автоматически определены.\n",
    "* `index_col`: По умолчанию None. Указывает номер столбца для использования в качестве индекса строк. Если не указано, индексы строк будут автоматически определены.\n",
    "* `skiprows`: По умолчанию None. Указывает номера строк для пропуска при загрузке таблицы.\n",
    "* `attrs`: По умолчанию None. Список атрибутов HTML-тега для использования в качестве фильтра при поиске таблиц.\n",
    "* `parse_dates`: По умолчанию False. Указывает, нужно ли парсить значения столбцов как даты.\n",
    "* `thousands`: По умолчанию ','. Определяет разделитель тысячных.\n",
    "* `encoding`: По умолчанию None. Указывает кодировку файла.\n",
    "* `decimal`: По умолчанию '.'. Определяет символ десятичного разделителя.\n",
    "* `converters`: По умолчанию None. Словарь, который определяет пользовательские функции преобразования для столбцов.\n",
    "* `na_values`: По умолчанию None. Список значений, которые должны быть распознаны как пропущенные значения.\n",
    "* `keep_default_na`: По умолчанию True. Определяет, должны ли использоваться значения по умолчанию для распознавания пропущенных значений.\n",
    "* `displayed_only`: По умолчанию True. Указывает, следует ли извлекать только видимые данные таблицы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример загрузки данных из локального html-файла\n",
    "import pandas as pd\n",
    "\n",
    "table_data = pd.read_html('путь_к_файлу.html')\n",
    "data_frame = table_data[0]\n",
    "\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример загрузки данных из HTML-страницы по URL-адресу\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.example.com/table.html'\n",
    "table_data = pd.read_html(url)\n",
    "data_frame = table_data[0]\n",
    "\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример загрузки данных с использованием фильтра атрибутов HTML-тега\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.example.com/table.html'\n",
    "table_data = pd.read_html(url, attrs={'class': 'table-class'})\n",
    "data_frame = table_data[0]\n",
    "\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример загрузки данных с парсингом дат и пользовательскими функциями преобразования\n",
    "import pandas as pd\n",
    "\n",
    "table_data = pd.read_html('путь_к_файлу.html', parse_dates=True, converters={'column_name': pd.to_numeric})\n",
    "data_frame = table_data[0]\n",
    "\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Разбор XML в Pandas**\n",
    "\n",
    "**XML формат**\n",
    "\n",
    "XML (eXtensible Markup Language) – это универсальный, масштабируемый, структурированный и текстовый формат данных, предназначенный для обмена данными между различными системами и приложениями. XML позволяет описать данные и их структуру с использованием вложенных тегов, атрибутов и текстового содержимого. Основной целью XML является передача данных, а не их отображение (в отличие от HTML, например).\n",
    "\n",
    "Преимущества XML:\n",
    "\n",
    "1. Человекочитаемость: XML-файлы легко читать и понимать для человека, поскольку они отображают структуру данных в виде текста, организованного с использованием вложенных тегов.\n",
    "2. Машинная обрабатываемость: XML-файлы могут быть легко обработаны различными парсерами и программами, которые способны обрабатывать иерархические структуры данных.\n",
    "3. Открытый формат: XML – это открытый стандарт, утвержденный и поддерживаемый W3C (World Wide Web Consortium). Это гарантирует широкую поддержку формата различными приложениями и платформами.\n",
    "4. Самодокументируемость: благодаря использованию разметки с именами тегов и атрибутов, которые описывают данные, XML обеспечивает некоторую степень самодокументации, что упрощает понимание структуры данных.\n",
    "5. Язык схем: XML может быть проверен на соответствие определенным схемам (например, DTD и XSD), что дает возможность проверки корректности структуры и содержимого файла.\n",
    "\n",
    "Важные концепции XML:\n",
    "\n",
    "1. Элементы: элементы XML – это основные единицы данных, обозначенные парами тегов: открывающего (`<element>`) и закрывающего (`</element>`). Элементы могут иметь вложенные элементы (дочерние).\n",
    "2. Атрибуты: атрибуты предоставляют дополнительную информацию об элементе, такую как метаданные, идентификаторы или свойства элемента. Атрибуты размещаются внутри открывающего тега элемента в формате name=\"value\".\n",
    "3. Текстовое содержимое: текстовое содержимое элемента располагается между открывающим и закрывающим тегами. Это может быть как простой текст, так и специальные символы, такие как сущности.\n",
    "4. Объявление XML: XML-файлы начинаются с объявления XML, которое определяет используемую версию XML и кодировку текста в файле. Пример: `<?xml version=\"1.0\" encoding=\"UTF-8\"?>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция read_xml(). Существует в Pandas с версии 1.3.\n",
    "\n",
    "Параметры:\n",
    "* `path_or_buffer`: Путь к XML-файлу, URL-адресу или файлоподобному объекту.\n",
    "* `xpath`: выражение XPath для выбора определенных частей XML для анализа.\n",
    "* `namespaces`: Словарь, содержащий пространство имен XML для уточнения выбора с помощью XPath.\n",
    "* `elems_only`: Если значение True, анализируются только текстовые значения элементов. Если значение False, анализируются как элементы, так и атрибуты.\n",
    "* `attrs_only`: Если значение True, анализируются только значения атрибутов. Если значение False, анализируются значения элементов.\n",
    "* `names`: список имен столбцов для результирующего фрейма данных.\n",
    "* `encoding`: тип кодировки для XML. По умолчанию используется UTF-8.\n",
    "* `parser`: используемый парсер XML (lxml или etree).\n",
    "* `stylesheet`: путь к файлу таблицы стилей XSLT для преобразования XML-данных перед парсингом.\n",
    "* `compression`: тип сжатия (‘infer’, ‘gzip’, ‘bz2’, ‘zip’, ‘xz’, нет). Если ‘infer’, то для определения типа сжатия используется расширение файла.\n",
    "* `storage_option`s: Дополнительные опции для подключения хранилища, если это необходимо.\n",
    "* `dtype_backend`: Серверная часть, используемая для вывода dtype (“python” или “lxml”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Файлы двоичных форматов в Pandas**\n",
    "\n",
    "**HDF5 (Hierarchical Data Format)** - это высокоэффективный, гибкий и платформонезависимый двоичный формат для хранения научных данных. Он хранит данные в иерархической структуре и использует сжатие данных для уменьшения размера файла. Основное преимущество этого формата - частичная загрузка и запись данных без необходимости считывать весь файл.\n",
    "\n",
    "Pandas предоставляет функции `read_hdf()` и `to_hdf()` для чтения и записи данных в формате HDF5. Для работы с HDF5 необходимо установить библиотеку tables.\n",
    "\n",
    "`read_hdf()` принимает на вход два основных аргумента:\n",
    "* `path_or_buf`: строка с путем к файлу или объект файла, содержащего данные в формате HDF5.\n",
    "* `key`: строка, указывающая ключ (имя) датасета в файле HDF5, который нужно прочитать.\n",
    "\n",
    "Дополнительные аргументы могут быть указаны для определения, как функция должна открыть файл, а также определения столбцов и фильтров для чтения.\n",
    "\n",
    "`to_hdf()` предназначена для записи данных из датафрейма Pandas в файл формата HDF5. Она вызывается как метод датафрейма и принимает два основных аргумента:\n",
    "\n",
    "* `path_or_buf`: строка с путем к файлу или объект файла, в котором требуется сохранить данные датафрейма в формате HDF5.\n",
    "* `key`: строка, указывающая ключ (имя) датасета, под которым будет сохранены данные датафрейма в файле HDF5.\n",
    "\n",
    "Дополнительные аргументы могут быть указаны для определения режима, в котором файл будет открыт, а также управления сжатием и хранением данных в файле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение данных из файла HDF5 \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_hdf('data.h5', key='dataset_name') \n",
    "# обработка данных \n",
    "# ... \n",
    "# запись результата обратно в файл HDF5 \n",
    "data.to_hdf('result.h5', key='new_dataset_name', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для частичной загрузки данных из HDF5-файла можно использовать метод read_hdf() вместе с дополнительными аргументами start, stop, который определяют диапазон индексов строк для чтения. Когда указываются start и stop, функция read_hdf() будет загружать только часть данных, соответствующую указанному диапазону строк. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение данных из файла HDF5 с указанным диапазоном строк \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_hdf('data.h5', key='dataset_name', start=10, stop=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parquet** - это эффективный двоичный формат для столбцовых данных, оптимизированный для больших объемов данных. Он использует сжатие данных и предоставляет возможность чтения и записи отдельных столбцов без необходимости считывать весь файл. Parquet часто используется с Apache Hadoop, Apache Spark и другими программными решениями для работы с большими данными.\n",
    "\n",
    "Pandas предоставляет функции read_parquet() и to_parquet() для чтения и записи данных в формате Parquet.\n",
    "\n",
    "Для работы с Parquet вам нужно установить библиотеку pyarrow или fastparquet. Fastparquet - это библиотека на чистом Python для работы с файлами формата Parquet, позволяющая считывать и записывать данные. Она обладает эффективными инструментами для работы с большим объемом данных и множеством типов колонок.\n",
    "Pyarrow - это библиотека Arrow для Python, которая предоставляет более широкие возможности для работы с файлами Parquet, включая совместимость с форматом Apache Arrow - кросс-языковой платформы для использования эффективных структур данных для работы с большим объемом данных в памяти.\n",
    "\n",
    "read_parquet() принимает несколько аргументов:\n",
    "\n",
    "* `path`: строка с путем к файлу в формате Parquet или объект файла.\n",
    "* `engine`: опциональный параметр, указывающий на движок, который будет использоваться для чтения данных. По умолчанию auto, что означает, что Pandas будет выбирать движок автоматически. Другие варианты включают pyarrow и fastparquet.\n",
    "* `columns`: список колонок для чтения. Если указаны столбцы, только они будут прочитаны из файла. По умолчанию - None, значит чтение всех колонок.\n",
    "* `use_nullable_dtypes`: если True, то используются типы данных, которые поддерживают значения NULL. По умолчанию False.\n",
    "* `**kwargs`: дополнительные параметры, передаваемые конкретному движку. Зависят от выбранного engine, и могут включать различные опции для чтения и обработки файла.\n",
    "\n",
    "to_parquet() - функция, которая позволяет сохранять объекты типа DataFrame как файлы в формате parquet. Основные параметры:\n",
    "\n",
    "* `path`: Путь к файлу, в который будет сохранен DataFrame.\n",
    "* `engine`: Библиотека ввода/вывода файла parquet, по умолчанию auto (автоматически выбирает между 'pyarrow' и 'fastparquet').\n",
    "* `compression`: Метод сжатия данных, например 'snappy', 'gzip', 'lzo', 'bz2', 'brotli', 'zstd' или 'uncompressed'. Значение по умолчанию - 'snappy'.\n",
    "* `index`: Сохранять ли индекс DataFrame в файле или нет. По умолчанию устанавливается в True.\n",
    "* `partition_cols`: Список столбцев, которые используются для разделения данных на разделы при сохранении. Если None (по умолчанию), данные записываются в единой файловой структуре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение данных из файла Parquet \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_parquet('data.parquet') \n",
    "# обработка данных \n",
    "# ... \n",
    "# запись результата обратно в файл Parquet \n",
    "data.to_parquet('result.parquet', engine='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MessagePack** - это компактный двоичный формат сериализации данных, который поддерживает большинство встроенных типов данных в Python и подходит для обмена данными между разными языками программирования. Он считался эффективной альтернативой JSON для хранения и передачи данных в двоичном формате. Pandas предлагает функции read_msgpack() и to_msgpack() для работы с данными в формате MessagePack.\n",
    "\n",
    "\n",
    "Для работы с MessagePack вам нужно установить библиотеку msgpack-python.\n",
    "\n",
    "Примечание: использование msgpack с Pandas было объявлено устаревшим с версии 1.0, и в будущих версиях установка msgpack не будет возможна по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение данных из файла MessagePack \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_msgpack('data.msg') \n",
    "# обработка данных \n",
    "# ... \n",
    "# запись результата обратно в файл MessagePack \n",
    "data.to_msgpack('result.msg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Чтение и запись файлов Microsoft Excel**\n",
    "\n",
    "Для работы с файлами Excel в pandas необходимо установить следующие пакеты:\n",
    "* openpyxl (для работы с .xlsx файлами),\n",
    "* xlrd (для работы с .xls файлами).\n",
    "\n",
    "Для чтения Excel-файла в pandas используется функция pd.read_excel(). Эта функция возвращает DataFrame, который содержит данные из файла Excel. Основные аргументы функции:\n",
    "\n",
    "* `io` : путь к файлу (строка) или объект файла.\n",
    "* `sheet_name`: имя листа с данными или его порядковый номер (нумерация с нуля). Если не указать этот аргумент, pandas прочитает только первый лист. Если задать значение None, то pandas прочитает все листы из файла и вернет словарь с датафреймами.\n",
    "* `header`: номер строки (или номера строк, если список), которые будут использованы как имена столбцов. Значение по умолчанию 0.\n",
    "* `index_col`: номер столбца (или номера столбцов, если список), который будет использован в качестве индекса строк. Значение по умолчанию None.\n",
    "* `usecols`: список (или строка) имен столбцов или номеров столбцов, которые будут загружены в датафрейм. Значение по умолчанию None (загрузятся все столбцы).\n",
    "\n",
    "Функция to_excel позволяет экспортировать данные из датафрейма Python в файл формата Microsoft Excel. Основные аргументы функции:\n",
    "* `excel_writer`: Строка с именем файла или объект типа ExcelWriter.\n",
    "* `sheet_name`: Имя листа, на который будет записан DataFrame. По умолчанию - 'Sheet1'.\n",
    "* `na_rep`: Строка, которая будет использоваться вместо NA/NaN. По умолчанию - пустая строка.\n",
    "* `float_format`: Формат для чисел с плавающей запятой. Например, \"%.2f\" для округления до двух знаков после запятой.\n",
    "* `columns`: Список столбцов, которые следует записать. Если не указано, будут записаны все столбцы.\n",
    "* `header`: Заголовки столбцов, которые должны быть сохранены (True) или строки, которые должны быть записаны вместо имен столбцов (список строк). Если не указано, сохраняются все заголовки.\n",
    "* `index`: Флаг, задающий сохранение (True, по умолчанию) или не сохранение (False) индекса строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример импорта DataFrame из Excel-файла с дополнительными аргументами и экспорта его обратно\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"example.xlsx\", sheet_name=\"Sheet2\", header=0, index_col=0, usecols=\"A,B,D\")\n",
    "\n",
    "df.to_excel(\"output.xlsx\", sheet_name=\"MyData\", na_rep=\"N/A\", float_format=\"%.2f\", columns=[\"A\", \"C\"], header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Работа с SQL базами данных в Pandas**\n",
    "\n",
    "## **Модуль SQLAlchemy в Pandas**\n",
    "\n",
    "**База данных (БД)** - это организованная коллекция данных, которая позволяет эффективно хранить, управлять, обновлять и извлекать информацию. Они широко применяются в различных областях, включая бизнес, науку, технологии и другие. Базы данных предоставляют удобные инструменты для структурирования данных и обеспечивают целостность и безопасность информации.\n",
    "\n",
    "Роль баз данных включает:\n",
    "* Хранение данных: БД предоставляют механизмы для долгосрочного хранения данных, обеспечивая их сохранность и доступность в будущем.\n",
    "* Управление данными: БД позволяют управлять данными, включая добавление, изменение и удаление информации. Они предоставляют средства для поддержки транзакций, целостности данных и контроля доступа.\n",
    "* Извлечение данных: БД предоставляют мощные средства для выполнения запросов к данным и извлечения нужной информации. Это позволяет анализировать данные, находить тренды и принимать обоснованные решения на основе информации.\n",
    "\n",
    "Для работы с базами данных в Pandas нам понадобится библиотека sqlalchemy - для установления соединения с базой данных и выполнения запросов. \n",
    "\n",
    "SQLAlchemy - это библиотека Python, которая предоставляет удобный и мощный способ взаимодействия с различными базами данных с использованием SQL. Она представляет собой ORM (Object-Relational Mapping), и позволяет работать с базами данных в объектно-ориентированной парадигме.\n",
    "\n",
    "Основные возможности SQLAlchemy:\n",
    "1. ORM-функциональность: SQLAlchemy позволяет создавать классы Python, которые отображаются на таблицы базы данных, что дает возможность работать с данными, как с объектами, а не с SQL-запросами. ORM-модель обеспечивает удобное взаимодействие с базой данных, скрывая детали низкоуровневого SQL-кода.\n",
    "2. Гибкость в выборе базы данных: SQLAlchemy поддерживает множество различных баз данных, включая SQLite, MySQL, PostgreSQL, Oracle, Microsoft SQL Server и другие. Это позволяет легко переключаться между разными базами данных без изменения кода.\n",
    "3. Создание и управление схемой базы данных: SQLAlchemy позволяет создавать таблицы и отношения между ними с помощью объявления моделей классов. Он предоставляет инструменты для миграции данных и изменения структуры базы данных.\n",
    "4. Поддержка транзакций: SQLAlchemy предоставляет механизм управления транзакциями, что позволяет выполнять группу операций базы данных как единую логическую единицу. Транзакции обеспечивают целостность данных и гарантируют, что изменения будут сохранены только в случае успешного завершения всех операций.\n",
    "5. Генерация SQL-запросов: SQLAlchemy предлагает высокоуровневый DSL (Domain Specific Language) для генерации SQL-запросов. Это позволяет строить сложные запросы с использованием высокоуровневых методов и операторов, что делает код более понятным и поддерживаемым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для соединения с базой данных необходимо будет установить соответствующий драйвер.\n",
    "* MySQL: Если планируется работа с базой данных MySQL, необходимо установить драйвер mysql-connector-python.\n",
    "* PostgreSQL: Если планируется работа с базой данных PostgreSQL, необходимо установить драйвер psycopg2.\n",
    "* SQLite: Python уже имеет встроенную поддержку SQLite, поэтому устанавливать отдельный драйвер для SQLite не нужно.\n",
    "\n",
    "Чтобы подключиться к базе данных, необходимо предоставить соединительную строку (connection string), которая содержит информацию о хосте, порте, имени пользователя, пароле и имени базы данных (если требуется). Формат соединительной строки зависит от типа базы данных.\n",
    "* MySQL: `engine = sqlalchemy.create_engine('mysql+mysqlconnector://user:password@host:port/database')`\n",
    "* PostgreSQL: `engine = sqlalchemy.create_engine('postgresql+psycopg2://user:password@host:port/database')`\n",
    "\n",
    "После создания объекта engine его можно использовать для установления соединения с базой данных.\n",
    "\n",
    "`connection = engine.connect()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Чтение данных из базы данных SQLite**\n",
    "\n",
    "### **Функция `read_sql_query()`**\n",
    "\n",
    "Одним из способов чтения данных из базы данных в Pandas является использование функции `read_sql_query()`. Эта функция позволяет выполнить произвольный SQL-запрос и получить результат в виде DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример использования функции read_sql_query\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Установка соединения с базой данных SQLite\n",
    "conn = sqlite3.connect('example.db')\n",
    "\n",
    "# SQL-запрос\n",
    "query = \"SELECT * FROM employees\"\n",
    "\n",
    "# Чтение данных в DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Закрытие соединения с базой данных\n",
    "conn.close()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека sqlite3 является стандартной библиотекой Python, предназначенной для работы с базами данных SQLite. Она предоставляет набор функций и классов, которые облегчают взаимодействие с базой данных SQLite из программы на Python. Для использования sqlite3 не требуется дополнительная установка, так как она является частью стандартной библиотеки Python. Её можно импортировать с помощью команды `import sqlite3`.\n",
    "\n",
    "Далее сначала устанавливается соединение с базой данных SQLite с использованием функцию sqlite3.connect. После выполнения этой строки переменная conn будет содержать объект соединения, который можно использовать для выполнения SQL-запросов и взаимодействия с базой данных SQLite.\n",
    "\n",
    "Строка query = \"SELECT * FROM employees\" представляет собой SQL-запрос на выборку данных из таблицы \"employees\",  которая указывает, какие данные необходимо выбрать из таблицы \"employees\". В данном случае SELECT * означает выбрать все столбцы, а FROM employees указывает, что мы выбираем данные из таблицы с именем \"employees\".\n",
    "\n",
    "С помощью функции pd.read_sql_query выполняется SQL-запрос и получается результат в виде DataFrame df. Функция read_sql_query автоматически преобразует результат запроса в структуру DataFrame, где каждая колонка соответствует столбцу в результирующем наборе, а каждая строка представляет одну запись данных.\n",
    "\n",
    "Синтаксис функции:\n",
    "\n",
    "`pd.read_sql_query(sql, con, params=None, index_col=None, coerce_float=True, parse_dates=None, chunksize=None)`\n",
    "\n",
    "Параметры:\n",
    "* `sql`: SQL-запрос, который определяет данные, которые требуется извлечь из базы данных.\n",
    "* `con`: Объект соединения с базой данных (например, созданный с помощью sqlite3.connect() или другими средствами).\n",
    "* `params` (опционально): Параметры, которые могут быть переданы в SQL-запрос в качестве аргументов.\n",
    "* `index_col` (опционально): Название столбца, который должен быть использован в качестве индекса (индекс столбцов DataFrame).\n",
    "* `coerce_float` (по умолчанию True): Если True, значения чисел преобразуются в тип float.\n",
    "* `parse_dates` (опционально): Столбцы, которые требуется распарсить как даты.\n",
    "* `chunksize` (опционально): Размер блока (чанка) данных для чтения пакетами.\n",
    "\n",
    "После чтения данных соединение с базой данных закрывается с помощью метода close(). Закрытие соединения с базой данных является хорошей практикой по работе с базами данных. Вот несколько причин, почему важно закрывать соединение с базой данных:\n",
    "1. Освобождение ресурсов: Когда закрывается соединение с базой данных, освобождаются ресурсы, занимаемые этим соединением, такие как сетевые ресурсы, память и блокировки. Это особенно важно в случае долгоживущих приложений или при работе с большим количеством соединений, чтобы избежать истощения системных ресурсов.\n",
    "2. Сохранение изменений: База данных может поддерживать механизмы транзакций, которые требуют явного фиксирования (commit) изменений, чтобы они были сохранены. Если не закрывается соединение с базой данных, изменения, выполненные в рамках текущей транзакции, могут быть потеряны или заблокированы для других пользователей базы данных.\n",
    "3. Предотвращение потенциальных проблем: Закрытие соединения помогает предотвратить потенциальные проблемы, связанные с утечками памяти или блокировками, особенно в случае работы с несколькими процессами или потоками. Закрытие соединения гарантирует, что ресурсы будут правильно освобождены и другие процессы или потоки смогут получить доступ к базе данных.\n",
    "4. Безопасность: Закрытие соединения с базой данных помогает обеспечить безопасность данных. При закрытии соединения есть возможность убедиться, что все незафиксированные изменения были сохранены или откатить их в случае необходимости.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Функция `read_sql_table()`**\n",
    "\n",
    "Функция `pd.read_sql_table()` позволяет прочитать данные из таблицы базы данных и вернуть результат в виде объекта DataFrame библиотеки Pandas. Она предоставляет удобный способ чтения всей таблицы целиком и преобразования ее в структуру DataFrame. Однако она не поддерживается стандартным модулем sqlite3, для ее использования необходимо подключать библиотеку sqlalchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример использования read_sql_table\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Создание подключения к базе данных с использованием SQLAlchemy\n",
    "engine = create_engine(\"sqlite:///example.db\")\n",
    "\n",
    "# Чтение таблицы в DataFrame\n",
    "table_name = \"employees\"\n",
    "df = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "# Закрытие соединения с базой данных\n",
    "engine.dispose()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В строке engine = create_engine(\"sqlite:///example.db\") используется функция create_engine из библиотеки SQLAlchemy для создания объекта \"engine\". Объект engine представляет собой подключение к базе данных и используется для отправки запросов и получения результатов работы с базой данных. Аргумент, передаваемый функции create_engine, является строкой подключения, которая определяет тип СУБД (в данном случае SQLite), и путь к файлу базы данных.\n",
    "\n",
    "Строка подключения \"sqlite:///example.db\" имеет следующую структуру:\n",
    "\n",
    "1. `sqlite`: указывает, что мы работаем с базой данных SQLite.\n",
    "2. `///`: определяет разделитель между типом базы данных и путём к файлу. SQLite является файловой базой данных, поэтому тройные слэши используются для указания относительного пути к файлу.\n",
    "3. `example.db`: имя файла базы данных, с которым будет установлено соединение. В данном случае, файл называется \"example.db\" и будет находиться в той же директории, где выполняется скрипт. Если файл базы данных находится в другой директории, нужно указать соответствующий путь.\n",
    "\n",
    "После создания объекта engine, его можно использовать в функции pd.read_sql_table для чтения таблицы из базы данных, также объект используется для других операций SQLAlchemy, таких как создание таблиц, вставка, обновление и удаление данных и т.д. .\n",
    "\n",
    "Затем используется pd.read_sql_table() для чтения данных из таблицы \"employees\" и преобразования их в DataFrame.\n",
    "\n",
    "Наконец, закрывается соединение с базой данных и выводятся первые несколько строк DataFrame. Важно не забывать закрыть соединение с базой данных после работы с ней, вызвав метод engine.dispose().\n",
    "\n",
    "Cинтаксис функции pd.read_sql_table:\n",
    "\n",
    "`pd.read_sql_table(table_name, con, schema=None, index_col=None, coerce_float=True, parse_dates=None, columns=None, chunksize=None)`\n",
    "\n",
    "Параметры:\n",
    "* `table_name`: (строка) Имя таблицы в базе данных, которую необходимо прочитать.\n",
    "* `con`: (объект SQLAlchemy engine или connection) Объект, представляющий соединение с базой данных. Обычно это объект engine или connection, созданный с помощью функции create_engine() или метода engine.connect() из библиотеки SQLAlchemy.\n",
    "* `schema`: (строка, опционально) Имя схемы в базе данных, из которой следует читать таблицу. Не все СУБД поддерживают схемы, если схема не указана, будет использоваться схема по умолчанию.\n",
    "* `index_col`: (строка или список строк, опционально) Один или несколько столбцов, которые будут использоваться в качестве индекса для полученного DataFrame. Если не указано, будет создан индекс по умолчанию.\n",
    "* `coerce_float`: (логический, опционально) Если True, все столбцы с типом данных FLOAT, которые не являются числами с плавающей запятой, будут преобразованы в числа с плавающей запятой. По умолчанию True.\n",
    "* `parse_dates`: (список, словарь или 'all', опционально) Столбцы для преобразования в тип данных datetime. Может быть списком столбцов, словарем с {'col_name': format}, где 'format' - строка формата или специальное значение 'all' для обработки всех столбцов.\n",
    "* `columns`: (список или подобный, опционально) Список столбцов для чтения из таблицы. Если не указано, будут прочитаны все столбцы.\n",
    "* `chunksize`: (целое число, опционально) Если указано, возвращает объект итератора, который читает данные по chunksize строк за раз, а не сразу весь DataFrame. Это полезно для обработки больших таблиц, когда необходимо снизить потребление памяти. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Функция `pd.read_sql()`**\n",
    "\n",
    "Функция `pd.read_sql()` является унивенсальным методом Pandas, который позволяет выполнить SQL-запрос к базе данных и вернуть результат в виде объекта DataFrame библиотеки Pandas. Она предоставляет мощный и гибкий способ чтения данных, позволяя выполнять различные типы SQL-запросов, включая сложные слияния таблиц, агрегирование, фильтрацию и другие операции.\n",
    "\n",
    "\n",
    "Синтаксис функции:\n",
    "\n",
    "`pd.read_sql(sql, con, index_col=None, coerce_float=True,  params=None, parse_dates=None, columns=None, chunksize=None)`\n",
    "\n",
    "Параметры:\n",
    "* `sql`: SQL-запрос, который определяет данные, которые требуется извлечь из базы данных.\n",
    "* `con`: Объект соединения с базой данных (например, созданный с помощью sqlite3.connect() или другими средствами).\n",
    "* `index_col` (опционально): Название столбца, который должен быть использован в качестве индекса (индекс столбцов DataFrame).\n",
    "* `coerce_float` (по умолчанию True): Если True, значения чисел преобразуются в тип float.\n",
    "* `params` (опционально): Параметры, которые могут быть переданы в SQL-запрос в качестве аргументов.\n",
    "* `parse_dates` (опционально): Столбцы, которые требуется распарсить как даты.\n",
    "* `columns` (опционально): Список столбцов, которые требуется прочитать из результатов запроса.\n",
    "* `chunksize` (опционально): Размер блока (чанка) данных для чтения пакетами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример использования pd.read_sql()\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Создание соединения с базой данных\n",
    "conn = sqlite3.connect('example.db')\n",
    "\n",
    "# Выполнение SQL-запроса и чтение данных в DataFrame\n",
    "query = \"SELECT * FROM employees WHERE age > 30\"\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# Закрытие соединения с базой данных\n",
    "conn.close()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере создается соединение с базой данных SQLite, используя sqlite3.connect(). Затем выполняется SQL-запрос \"SELECT * FROM employees WHERE age > 30\", который выбирает все строки из таблицы \"employees\", где значение столбца \"age\" больше 30. Результат запроса сохраняется в DataFrame с помощью pd.read_sql(). Заключительно, закрывается соединение с базой данных и выводятся первые несколько строк DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Сравнение функций чтения из базы данных SQLite**\n",
    "\n",
    "Отличие функции pd.read_sql() от функций pd.read_sql_table() и pd.read_sql_query() заключается в способе, которым они читают данные из базы данных и вводят SQL-запросы.\n",
    "\n",
    "`pd.read_sql_table()`:\n",
    "1. Читает данные из всей таблицы целиком.\n",
    "2. Автоматически создает SQL-запрос для выборки всех столбцов из указанной таблицы.\n",
    "3. Удобна в использовании, когда требуется прочитать все данные из таблицы без необходимости создавать сложные SQL-запросы.\n",
    "\n",
    "`pd.read_sql_query()`:\n",
    "1. Позволяет выполнять произвольные SQL-запросы для извлечения данных из базы данных.\n",
    "2. Позволяет выбирать конкретные столбцы, применять фильтры, сортировку и другие операции, поддерживаемые SQL.\n",
    "3. Предоставляет более гибкий подход, когда требуется выполнить сложные SQL-запросы или получить только определенные данные из таблицы.\n",
    "\n",
    "`pd.read_sql()`:\n",
    "1. Позволяет выполнить SQL-запрос и получить результаты в виде объекта DataFrame.\n",
    "2. Позволяет выполнять произвольные SQL-запросы и обрабатывать результаты.\n",
    "3. Обеспечивает более гибкий и мощный подход, чем pd.read_sql_table() и pd.read_sql_query().\n",
    "4. Может быть использован для чтения данных из таблицы целиком или для выполнения сложных запросов с фильтрацией, сортировкой, объединением таблиц и другими операциями.\n",
    "\n",
    "Выбор между этими функциями зависит от требований проекта и типа операций, которые необходимо выполнить с данными из базы данных. Если нужно прочитать данные из таблицы целиком, то pd.read_sql_table() может быть предпочтительным. Если требуется выполнить произвольный SQL-запрос, выбрать определенные столбцы или применить фильтрацию, то pd.read_sql_query() или pd.read_sql() предоставят большую гибкость и функциональность."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
